{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 1:\n",
        "\n",
        "Escriba ejemplos de frase o busque ejemplos de párrafos de texto con diferentes estructuras, palabras y signos de puntuación. Utilice los códigos de One-Hot Encoding en las dos versiones que presenta el material expuesto en clase. Analice los resultados."
      ],
      "metadata": {
        "id": "bM2sPewkfdCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D337qe8XfYUp"
      },
      "outputs": [],
      "source": [
        "#importo las librerias necesarias\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Mis frases que luego divido en palabras\n",
        "fraseUno = \"Estaba en llamas cuando me acosté.\"\n",
        "fraseDos = \"La guitarra es un instrumento musical.\"\n",
        "\n",
        "palabrasUno = fraseUno.split()\n",
        "palabrasDos = fraseDos.split()\n",
        "\n",
        "#-----------Primera-Frase--------------------------------------------------------------------------\n",
        "\n",
        "#OneHotEncoding con OneHotEncoder\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "onehot_encodedUno = onehot_encoder.fit_transform(np.array(palabrasUno).reshape(-1, 1))\n",
        "\n",
        "print(onehot_encodedUno)\n",
        "\n",
        "for i, palabra in enumerate(palabrasUno):\n",
        "    print(f\"La palabra '{palabra}' se codificó como: {onehot_encodedUno[i]}\")\n",
        "\n",
        "#OneHotEncoding con pandas\n",
        "dataFrame = pd.DataFrame(palabrasUno, columns=['Palabras'])\n",
        "\n",
        "onehot_encodedDos = pd.get_dummies(dataFrame['Palabras'])\n",
        "\n",
        "print(onehot_encodedDos)\n",
        "\n",
        "for i, palabra in enumerate(palabrasUno):\n",
        "    print(f\"La palabra '{palabra}' se codificó como: {onehot_encodedDos.iloc[i].to_numpy()}\")\n",
        "\n",
        "#-----------Segunda-Frase--------------------------------------------------------------------------\n",
        "\n",
        "#OneHotEncoding con OneHotEncoder\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "onehot_encodedUno = onehot_encoder.fit_transform(np.array(palabrasDos).reshape(-1, 1))\n",
        "\n",
        "print(onehot_encodedUno)\n",
        "\n",
        "for i, palabra in enumerate(palabrasDos):\n",
        "    print(f\"La palabra '{palabra}' se codifió como: {onehot_encodedUno[i]}\")\n",
        "\n",
        "#OneHotEncoding con pandas\n",
        "dataFrame = pd.DataFrame(palabrasDos, columns=['Palabras'])\n",
        "\n",
        "onehot_encodedDos = pd.get_dummies(dataFrame['Palabras'])\n",
        "\n",
        "print(onehot_encodedDos)\n",
        "\n",
        "for i, palabra in enumerate(palabrasDos):\n",
        "    print(f\"La palbra '{palabra}' se codificó como: {onehot_encodedDos.iloc[i].to_numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 2:\n",
        "\n",
        "De los recursos propuestos en el último ejercicio de la unidad 1 con el texto obtenido del documento PROYECTO DE LEY TURISMO SOCIAL 2004.pdf y el texto de la metodología en la extracción de webscrapping del ministerio de turismo.\n",
        "\n",
        "Utilizar las bibliotecas de procesamiento de texto en python para representar estos documentos en forma de matrices numéricas utilizando tanto CountVectorizer como TfidfVectorizer, y luego comparar las diferencias entre las dos representaciones.\n",
        "\n",
        "Si es necesario eliminar en alguno de los casos las \"stop-words\"."
      ],
      "metadata": {
        "id": "igZZJjjxpkkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "bBQkxh09cL-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "dw7_D5fyZ7n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-features"
      ],
      "metadata": {
        "id": "BVb-77XXav8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "fKQidsnWgm2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords_esp = set(stopwords.words('spanish'))"
      ],
      "metadata": {
        "id": "bIJHTqqkf_kE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3db928d7-0759-461b-fd69-470298737ddd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PROYECTO-DE-LEY-TURISMO-SOCIAL-2004.pdf-----Con-CountVectorizer-------------------------------------------------------------------------\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "with open('/content/PROYECTO DE LEY TURISMO SOCIAL 2004.pdf', 'rb'):\n",
        "    lector = PyPDF2.PdfReader('PROYECTO DE LEY TURISMO SOCIAL 2004.pdf')\n",
        "\n",
        "texto = \"\"\n",
        "\n",
        "for i in range(len(lector.pages)):\n",
        "    pagina = lector.pages[i]\n",
        "    texto += pagina.extract_text()\n",
        "\n",
        "#Tokenizo el texto y elimino las stopwords, luego genero el texto sin las stopwords:\n",
        "palabras = nltk.word_tokenize(texto)\n",
        "palabras_sin_stopwords = [palabra for palabra in palabras if palabra.lower() not in stopwords_esp]\n",
        "texto_sin_stopwords = ' '.join(palabras_sin_stopwords)\n",
        "\n",
        "#Segmentación del texto sin stopwords:\n",
        "corpus = texto_sin_stopwords.split(\".\")\n",
        "\n",
        "#Instancia de CountVectorizer:\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Vectores de características:\\n\", X.toarray())\n",
        "\n",
        "print(\"\\nPalabras del vocabulario:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "print('\\nVectores con palabras como columnas:')\n",
        "print(df)\n",
        "\n",
        "# Webscrapping-Ministerio-De-Turismo----------TfidfVectorizer-------------------------------------------------------------------------------\n",
        "\"\"\"\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://tableros.yvera.tur.ar/tablero_ODS/\"\n",
        "\n",
        "response = requests.get(url, verify=False)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "uls = soup.find_all('ul')\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for ul in uls:\n",
        "    descripciones = ul.find_all('p')\n",
        "    for d in descripciones:\n",
        "        corpus.append(d.text)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Ajustamos y transformamos nuestro corpus\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Características: \", vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nMatriz TF-IDF:\")\n",
        "print(X.toarray())\n",
        "\n",
        "# Resumen\n",
        "print(\"\\nVocabulario:\")\n",
        "print(vectorizer.vocabulary_)\n",
        "print(\"\\nIDF:\")\n",
        "print(vectorizer.idf_)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RHz4mnpnq1Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 3:\n",
        "\n",
        "Utilice la librería HashingVectorizer para obtener los vectores de\n",
        "características resultantes en los documentos anteriores:\n",
        "\n",
        "**¿Qué quiere decir que esta metodología es una técnica “sin estado”?**\n",
        "\n",
        "Significa que no mantiene ninguna información sobre el estado anterior (como un vocabulario). Esto significa que no puede proporcionar una forma de mapear desde las características a las palabras originales. Esto puede ser un inconveniente si necesitamos interpretar los vectores de características.\n",
        "\n",
        "**¿Cuándo puede ser una ventaja el uso de esta técnica?**\n",
        "\n",
        "Como no requiere que se mantenga un vocabulario, lo que puede ser muy útil en situaciones en las que el vocabulario puede ser muy grande y consumir mucha memoria.\n",
        "\n",
        "**¿En qué casos puede generar algunos problemas?**\n",
        "\n",
        "Cuando puede haber colisiones de hash y cuando no se puede mapear las características de vuelta a las palabras originales.\n",
        "\n",
        "**¿Explique el concepto de “matriz dispersa”?**\n",
        "\n",
        "La matriz dispersa es una matriz en la que la mayoría de los elementos son cero donde se puede tomar como criterio que la cantidad de elementos distintos de cero es aproximadamente igual al número de filas o columnas.\n",
        "\n",
        "**¿Qué rango numérico tienen los valores si se utiliza norm=None?**\n",
        "\n",
        "Cuando se utiliza HashingVectorizer con norm=None, el rango numérico de los valores resultantes es de 0 a n_features - 1, donde n_features es el tamaño del vector de características especificado."
      ],
      "metadata": {
        "id": "qpev_9JQeQre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "import PyPDF2\n",
        "\n",
        "#Vectorizacion Hash de PROYECTO DE LEY TURISMO SOCIAL 2004\n",
        "with open('/content/PROYECTO DE LEY TURISMO SOCIAL 2004.pdf', 'rb'):\n",
        "    lector = PyPDF2.PdfReader('PROYECTO DE LEY TURISMO SOCIAL 2004.pdf')\n",
        "\n",
        "texto = \"\"\n",
        "\n",
        "for i in range(len(lector.pages)):\n",
        "    pagina = lector.pages[i]\n",
        "    texto += pagina.extract_text()\n",
        "\n",
        "#Tokenizo el texto y elimino las stopwords, luego genero el texto sin las stopwords:\n",
        "palabras = nltk.word_tokenize(texto)\n",
        "palabras_sin_stopwords = [palabra for palabra in palabras if palabra.lower() not in stopwords_esp]\n",
        "texto_sin_stopwords = ' '.join(palabras_sin_stopwords)\n",
        "\n",
        "#Segmentación del texto sin stopwords:\n",
        "corpus = texto_sin_stopwords.split(\".\")\n",
        "\n",
        "#Este es el corpus:\n",
        "print(corpus)\n",
        "\n",
        "#Creo el Vectorizador Hash y aplico la transformación\n",
        "vectorizer = HashingVectorizer(n_features=10)\n",
        "vector_con_hash = vectorizer.transform(corpus)\n",
        "\n",
        "# Imprimimos el resultado\n",
        "print(vector_con_hash.toarray())\n",
        "\n",
        "# summarize the vector\n",
        "print('\\nForma del vector:')\n",
        "print(vector_con_hash.shape)"
      ],
      "metadata": {
        "id": "LGn0LnwbemMk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b98aee-fb4d-49f1-9312-ab17fb6f43dd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ley Nacional Turismo Social Título 1 Turismo Social Capitulo 1 Disposiciones generales ARTICULO l', '- Turismo social ', ' garantizará habitantes recursos económicos limitados , derecho descanso tiempo libre , permitiéndoles acceder distintas zonas turísticas , afianzando sentimiento pertenencia ', ' turismo social comprende aquellos instrumentos medios otorguen facilidades personas recursos económicos limitados , capacidades dl $ zrentes , accedan viajes turísticos fines recreativos condiciones adecuadas economía , seguridad comodidad ', ' ARTICULO 2 : Finalidad ', ' presente ley finalidad 4 Elaborar plan estratégico nacional turismo social “ PENTS ” b ) Promover desarrollo mejoramiento turismo social ; c ) Fomentar ejercicio turismo social , conservando preservando patrimonio natural , histórico cultural país ', ' Elaborando cada caso estudios impacto ambiental necesarios ', ' 4 Garantizar personas capacidades diferentes acceso conocimiento disfrute bellezas naturales pais ', ' Mantener mejorar oferta turismo social f , Desarrollar programas investigación junto unidades academicas nacionales respondan objetivos PENTS ', ' ARTICULO 3', '- Autoridad Aplicación podrá suscribir acuerdos prestadores servicios turísticos , organizaciones sociales empresas privadas fin analizar , evaluar determinar precios condiciones especiales favorables , posibiliten cumplimiento objetivos presente Título , beneficio sectores sociales recursos económicos limitados personas capacidades distintas ', ' ARTICULO 4 : Consejo Federal Turismo junto Autoridad Aplicación elaborarán Plan Estratégico Nacional Turismo Social , contará colaboración asesoramiento representantes técnicos cada provincia enviara Unidades Académicas Turísticas Pais ', ' ARTICULO 5 : Plan Estratégico Nacional Turismo social , contendrá estrategias promoción ejecución , creación colonias vacaciones , balnearios , campos deportivos recreación , demás establecimientos destinados turismo económico ; organización excursiones económicas ', ' ARTICULO 6 : Oferta relacionada turismo social mantendrá mejorara travks unidades propias ', ' ARTICULO 7 : Autoridad Aplicación junto Unidades Académicas Turísticas País , elaboraran sistema pasantías , destinados estudiantes universitarios terciarios distintas disciplinas turísticas , desempeñarse administración , funcionamiento distintas unidades turísticas ', ' ARTICULO 8 : beneficiarios prioritarios nifios edad escolar , jóvenes , ancianos , personas capacidades diferentes familias carenciadas ', ' ARTICULO 9 : entidades desarrollen presten actividades turismo recreación , deberán incorporar planes servicios , tratamiento preferencial personas tercera edad capacidades distintas ', ' ARTICULO 10 : Estímulos ', ' autoridad aplicación estimula inversiones tiendan incrementar instalaciones destinadas turismo social , así formación sociedades cooperativas tipo asociaciones auspicien forma turismo ', ' Capitulo Il Régimen Subsidios ARTICULO ll : Modalidades Subsidios Plenos : ) Autoridad Aplicación brindará gratuitamente , servicio alojamiento , comida recreación , escuelas públicas , asilos publico ancianos instituciones publicas protección ayuda personas capacidades especiales , mediante celebración ejecución convenios firmara Provincias , Municipios , obligarán efectuar prestación servicio transporte ', ' b ) Autoridad Aplicación brindará gratuitamente , servicio alojamiento , comida , recreación transporte personas capacidades diferentes ', ' Subsidios parciales : ) familias mas 2 hijos , solo deberán abonar importe correspondiente padres hijo , prestándose servicio turístico totalidad grupo familiar ', ' ARTICULO 12 : - PRESUPUESTO : Conformación Presupuesto Especial ) Asignación Presupuesto autoridad Aplicación b ) 25 % producto impuesto producido cinco ciento ( 5 % ) precio pasajes aéreos fluviales exterior , vendidos emitidos país vendidos emitidos exterior residentes argentinos viajes inicien territorio nacional ; c ) aporte realicen provincias d ) aporte Ministerio Desarrollo Social ', ' ) aporte distintos Organismos Nacionales extranjeros ', ' t ) cualquier ingreso genere leyes fondos especiales Título ll Infracciones Sanciones ARTíCULO 13', '- Infracciones ', ' ejercicio funciones contralor , Autoridad Aplicación puede imponer sanciones infracción ylo inobservancia presente Ley reglamentos consecuencia dicten ', ' ARTÍCULO 14', '- Sanciones ', ' aplicación sanciones previstas artículo precedente obsta suspensión , revocación caducidad autorizaciones administrativas otorgadas ', ' ARTíWLO 15', '- Procedimiento ', ' sanciones aplican mediante procedimiento establezca reglamentación presente Ley , perjuicio aplicación supletoria cuanto compatible disposiciones I , ey Nacional Procedimiento Administrativos N ” 19', '549 ', ' Titulo III Disposiciones Complementarias ARTICULO 16', '- Autoridad Aplicación ', ' SECRETARIA TURISMO PRESIDENCIA NACIÓN organismo futuro reemplace , Autoridad Aplicación presente norma , así disposiciones reglamentarias complementarias ', ' ARTICULO 17', '- Reglamentación ', ' presente ley debe ser reglamentada dentro ciento ochenta ( 180 ) días promulgación , ARTÍCULO 18', '- forma ', '']\n",
            "[[ 0.60302269  0.          0.30151134  0.         -0.30151134  0.\n",
            "   0.30151134  0.          0.60302269  0.        ]\n",
            " [ 0.70710678  0.          0.          0.         -0.70710678  0.\n",
            "   0.          0.          0.          0.        ]\n",
            " [-0.45883147  0.22941573  0.22941573  0.         -0.22941573  0.22941573\n",
            "  -0.22941573  0.          0.6882472   0.22941573]\n",
            " [ 0.16903085  0.16903085 -0.16903085  0.16903085 -0.50709255  0.16903085\n",
            "  -0.3380617  -0.50709255  0.3380617   0.3380617 ]\n",
            " [ 0.          0.          0.          0.          0.70710678  0.\n",
            "   0.          0.          0.         -0.70710678]\n",
            " [ 0.56568542  0.          0.          0.14142136 -0.56568542  0.\n",
            "   0.56568542  0.         -0.14142136  0.        ]\n",
            " [ 0.          0.4472136   0.4472136   0.          0.4472136   0.\n",
            "   0.          0.4472136   0.         -0.4472136 ]\n",
            " [ 0.          0.         -0.31622777  0.31622777  0.          0.31622777\n",
            "  -0.31622777 -0.63245553  0.31622777 -0.31622777]\n",
            " [-0.25819889  0.25819889  0.25819889 -0.77459667  0.          0.25819889\n",
            "  -0.25819889  0.          0.          0.25819889]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.         -1.        ]\n",
            " [ 0.23570226  0.47140452  0.23570226  0.23570226 -0.47140452  0.23570226\n",
            "  -0.23570226 -0.47140452  0.23570226  0.        ]\n",
            " [ 0.32025631  0.32025631  0.16012815 -0.32025631 -0.64051262  0.16012815\n",
            "  -0.16012815  0.32025631  0.32025631  0.        ]\n",
            " [ 0.43643578 -0.21821789 -0.21821789  0.         -0.65465367  0.43643578\n",
            "  -0.21821789  0.21821789  0.          0.        ]\n",
            " [ 0.         -0.40824829  0.         -0.40824829 -0.40824829  0.\n",
            "  -0.40824829  0.40824829  0.         -0.40824829]\n",
            " [ 0.          0.28867513 -0.14433757 -0.57735027 -0.14433757 -0.4330127\n",
            "  -0.57735027  0.14433757  0.          0.        ]\n",
            " [ 0.37796447 -0.37796447  0.          0.37796447  0.          0.37796447\n",
            "  -0.37796447 -0.37796447 -0.37796447  0.        ]\n",
            " [ 0.31622777 -0.31622777 -0.31622777  0.63245553  0.         -0.31622777\n",
            "  -0.31622777  0.31622777  0.          0.        ]\n",
            " [-0.57735027  0.          0.          0.          0.          0.\n",
            "   0.         -0.57735027  0.         -0.57735027]\n",
            " [ 0.          0.17407766  0.52223297 -0.34815531 -0.34815531  0.17407766\n",
            "   0.52223297  0.34815531  0.         -0.17407766]\n",
            " [-0.11250879  0.          0.22501758  0.22501758  0.11250879 -0.78756153\n",
            "  -0.33752637  0.11250879  0.11250879 -0.33752637]\n",
            " [ 0.         -0.5         0.5         0.          0.          0.\n",
            "  -0.5         0.          0.          0.5       ]\n",
            " [-0.31622777  0.31622777  0.          0.          0.31622777  0.31622777\n",
            "  -0.31622777  0.         -0.63245553 -0.31622777]\n",
            " [-0.16666667 -0.33333333  0.33333333 -0.16666667 -0.33333333  0.33333333\n",
            "  -0.16666667  0.          0.66666667 -0.16666667]\n",
            " [ 0.         -0.57735027  0.          0.          0.57735027 -0.57735027\n",
            "   0.          0.          0.          0.        ]\n",
            " [ 0.42640143 -0.21320072  0.         -0.21320072  0.21320072 -0.63960215\n",
            "   0.          0.21320072  0.42640143 -0.21320072]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.         -1.        ]\n",
            " [-0.26726124  0.26726124  0.         -0.26726124 -0.53452248  0.\n",
            "   0.26726124  0.26726124 -0.26726124  0.53452248]\n",
            " [ 0.         -0.70710678  0.          0.          0.          0.\n",
            "   0.         -0.70710678  0.          0.        ]\n",
            " [ 0.          0.          0.          0.          0.         -1.\n",
            "   0.          0.          0.          0.        ]\n",
            " [ 0.23570226 -0.47140452 -0.23570226  0.          0.         -0.70710678\n",
            "  -0.23570226  0.23570226  0.         -0.23570226]\n",
            " [-0.70710678  0.         -0.70710678  0.          0.          0.\n",
            "   0.          0.          0.          0.        ]\n",
            " [ 1.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]\n",
            " [ 0.48507125  0.          0.48507125  0.         -0.48507125 -0.24253563\n",
            "   0.24253563 -0.24253563  0.24253563  0.24253563]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          1.        ]\n",
            " [ 0.          0.          0.40824829  0.          0.          0.\n",
            "   0.          0.          0.40824829 -0.81649658]\n",
            " [ 0.          0.          0.         -0.70710678  0.          0.\n",
            "   0.          0.70710678  0.          0.        ]\n",
            " [ 0.30151134  0.          0.30151134 -0.30151134 -0.60302269  0.30151134\n",
            "   0.          0.30151134 -0.30151134 -0.30151134]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.70710678  0.          0.         -0.70710678]\n",
            " [ 0.          0.          0.          0.         -1.          0.\n",
            "   0.          0.          0.          0.        ]\n",
            " [-0.37139068 -0.37139068  0.18569534  0.18569534 -0.18569534 -0.18569534\n",
            "   0.18569534  0.          0.74278135  0.        ]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.         -1.        ]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]]\n",
            "\n",
            "Forma del vector:\n",
            "(42, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 4:\n",
        "\n",
        "**¿A qué hace referencia la bibliografía cuando indica que los métodos\n",
        "utilizados en los ejercicios anteriores no capturan la semántica y el contexto de las palabras.? De una explicación breve.**\n",
        "\n",
        "Cuando se dice que los métodos anteriores de vectorización no capturan la semántica y el contexto de las palabras es porque tratan a las palabras como entidades aisladas y no capturan el contexto o la relación entre esas palabras."
      ],
      "metadata": {
        "id": "dYhLc4PEemmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 5:\n",
        "\n",
        "Genere un código en python que permita tomar el resultado de una\n",
        "matriz de vectores de los ejercicios anteriores y extraiga las 10 similaridades de coseno mayores e imprima las palabras correspondientes en los casos que es\n",
        "posible.\n"
      ],
      "metadata": {
        "id": "r17bBYfjewMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(A, B):\n",
        "    \"\"\"\n",
        "    Calcula la similitud del coseno entre dos vectores A y B.\n",
        "\n",
        "    Parámetros:\n",
        "    - A, B: Vectores de entrada.\n",
        "\n",
        "    Retorna:\n",
        "    - Similitud del coseno entre A y B.\n",
        "    \"\"\"\n",
        "    dot_product = np.dot(A, B)\n",
        "    norm_A = np.linalg.norm(A)\n",
        "    norm_B = np.linalg.norm(B)\n",
        "\n",
        "    return dot_product / (norm_A * norm_B)\n",
        "\n",
        "num_words = len(vector_con_hash.toarray())\n",
        "similarities = np.zeros((num_words, num_words))\n",
        "\n",
        "for i in range(num_words):\n",
        "    for j in range(num_words):\n",
        "        if i != j:\n",
        "            similarity = cosine_similarity(vector_con_hash.toarray()[i], vector_con_hash.toarray()[j])\n",
        "            similarities[i, j] = similarity\n",
        "\n",
        "# Encontrar las 10 mayores similaridades de coseno\n",
        "top_similarities = []\n",
        "for i in range(num_words):\n",
        "    for j in range(i + 1, num_words):\n",
        "        top_similarities.append((similarities[i, j], i, j))\n",
        "\n",
        "top_similarities.sort(reverse=True)\n",
        "top_similarities = top_similarities[:10]\n",
        "\n",
        "# Imprimir las palabras correspondientes a las 10 mayores similaridades\n",
        "for similarity, idx1, idx2 in top_similarities:\n",
        "    word1 = palabras[idx1] if idx1 < len(palabras) else \"Palabra no encontrada\"\n",
        "    word2 = palabras[idx2] if idx2 < len(palabras) else \"Palabra no encontrada\"\n",
        "    print(f\"Similaridad de coseno: {similarity}, Palabras: {word1}, {word2}\")"
      ],
      "metadata": {
        "id": "isidV8CKmRrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 6:\n",
        "\n",
        "Cargue el modelo Word2Vec y explore palabras y similitudes en\n",
        "palabras que sean de su interés. Busque posibilidades y combinaciones donde el\n",
        "uso de la aritmética de palabras de resultados semánticamente coherentes."
      ],
      "metadata": {
        "id": "uRy5cJvb6mEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!wget https://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.bin.gz"
      ],
      "metadata": {
        "id": "sUYGa2S-6smv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "#Cargo un Modelo Word2Vec pre-entrenado.\n",
        "modelo = KeyedVectors.load_word2vec_format('SBW-vectors-300-min5.bin.gz', binary=True)\n",
        "\n",
        "#Información del modelo.\n",
        "print(modelo)\n",
        "\n",
        "#Muestro la similitud entre dos palabras.\n",
        "print(f\"Similitud entre dos palabras 'perro' y 'ladrar': {modelo.similarity('perro', 'ladrar')}\")\n",
        "print(f\"Similitud entre dos palabras 'gato' y 'mesa': {modelo.similarity('gato', 'mesa')}\")\n",
        "print(f\"Similitud entre dos palabras 'izquierda' y 'norte': {modelo.similarity('izquierda', 'norte')}\")"
      ],
      "metadata": {
        "id": "zv8aIvxGozJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 7:\n",
        "\n",
        "Utilice el texto recuperado del autor Hernán Casciari de la práctica\n",
        "anterior para crear un corpus de texto, elimine las stopwords y utilice Word2Vec, GloVe (Global Vectors for Word Representation) y FastText, compare los\n",
        "resultados.\n"
      ],
      "metadata": {
        "id": "VHod8MVg6s-f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_kM-ddx461zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 8:\n",
        "\n",
        "Separe por párrafos u oraciones el texto recopilado de Casciari. Usar\n",
        "doc2vec sobre este corpus."
      ],
      "metadata": {
        "id": "BYJiAhKM61Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7wc6XHLS69HP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}