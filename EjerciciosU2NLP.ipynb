{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 1:\n",
        "\n",
        "Escriba ejemplos de frase o busque ejemplos de párrafos de texto con diferentes estructuras, palabras y signos de puntuación. Utilice los códigos de One-Hot Encoding en las dos versiones que presenta el material expuesto en clase. Analice los resultados."
      ],
      "metadata": {
        "id": "bM2sPewkfdCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D337qe8XfYUp"
      },
      "outputs": [],
      "source": [
        "#importo las librerias necesarias\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Mis frases que luego divido en palabras\n",
        "fraseUno = \"Estaba en llamas cuando me acosté.\"\n",
        "fraseDos = \"La guitarra es un instrumento musical.\"\n",
        "\n",
        "palabrasUno = fraseUno.split()\n",
        "palabrasDos = fraseDos.split()\n",
        "\n",
        "#-----------Primera-Frase--------------------------------------------------------------------------\n",
        "\n",
        "#OneHotEncoding con OneHotEncoder\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "onehot_encodedUno = onehot_encoder.fit_transform(np.array(palabrasUno).reshape(-1, 1))\n",
        "\n",
        "print(onehot_encodedUno)\n",
        "\n",
        "for i, palabra in enumerate(palabrasUno):\n",
        "    print(f\"La palabra '{palabra}' se codificó como: {onehot_encodedUno[i]}\")\n",
        "\n",
        "#OneHotEncoding con pandas\n",
        "dataFrame = pd.DataFrame(palabrasUno, columns=['Palabras'])\n",
        "\n",
        "onehot_encodedDos = pd.get_dummies(dataFrame['Palabras'])\n",
        "\n",
        "print(onehot_encodedDos)\n",
        "\n",
        "for i, palabra in enumerate(palabrasUno):\n",
        "    print(f\"La palabra '{palabra}' se codificó como: {onehot_encodedDos.iloc[i].to_numpy()}\")\n",
        "\n",
        "#-----------Segunda-Frase--------------------------------------------------------------------------\n",
        "\n",
        "#OneHotEncoding con OneHotEncoder\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "onehot_encodedUno = onehot_encoder.fit_transform(np.array(palabrasDos).reshape(-1, 1))\n",
        "\n",
        "print(onehot_encodedUno)\n",
        "\n",
        "for i, palabra in enumerate(palabrasDos):\n",
        "    print(f\"La palabra '{palabra}' se codifió como: {onehot_encodedUno[i]}\")\n",
        "\n",
        "#OneHotEncoding con pandas\n",
        "dataFrame = pd.DataFrame(palabrasDos, columns=['Palabras'])\n",
        "\n",
        "onehot_encodedDos = pd.get_dummies(dataFrame['Palabras'])\n",
        "\n",
        "print(onehot_encodedDos)\n",
        "\n",
        "for i, palabra in enumerate(palabrasDos):\n",
        "    print(f\"La palbra '{palabra}' se codificó como: {onehot_encodedDos.iloc[i].to_numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 2:\n",
        "\n",
        "De los recursos propuestos en el último ejercicio de la unidad 1 con el texto obtenido del documento PROYECTO DE LEY TURISMO SOCIAL 2004.pdf y el texto de la metodología en la extracción de webscrapping del ministerio de turismo.\n",
        "\n",
        "Utilizar las bibliotecas de procesamiento de texto en python para representar estos documentos en forma de matrices numéricas utilizando tanto CountVectorizer como TfidfVectorizer, y luego comparar las diferencias entre las dos representaciones.\n",
        "\n",
        "Si es necesario eliminar en alguno de los casos las \"stop-words\"."
      ],
      "metadata": {
        "id": "igZZJjjxpkkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "bBQkxh09cL-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "dw7_D5fyZ7n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-features"
      ],
      "metadata": {
        "id": "BVb-77XXav8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "\n",
        "stopwords_esp = set(stopwords.words('spanish'))"
      ],
      "metadata": {
        "id": "bIJHTqqkf_kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROYECTO-DE-LEY-TURISMO-SOCIAL-2004.pdf-----Con-CountVectorizer-------------------------------------------------------------------------\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "with open('/content/PROYECTO DE LEY TURISMO SOCIAL 2004.pdf', 'rb'):\n",
        "    lector = PyPDF2.PdfReader('PROYECTO DE LEY TURISMO SOCIAL 2004.pdf')\n",
        "\n",
        "texto = \"\"\n",
        "\n",
        "for i in range(len(lector.pages)):\n",
        "    pagina = lector.pages[i]\n",
        "    texto += pagina.extract_text()\n",
        "\n",
        "#Tokenizo el texto y elimino las stopwords, luego genero el texto sin las stopwords:\n",
        "palabras = nltk.word_tokenize(texto)\n",
        "palabras_sin_stopwords = [palabra for palabra in palabras if palabra.lower() not in stopwords_esp]\n",
        "texto_sin_stopwords = ' '.join(palabras_sin_stopwords)\n",
        "\n",
        "#Segmentación del texto sin stopwords:\n",
        "corpus = texto_sin_stopwords.split(\".\")\n",
        "\n",
        "#Instancia de CountVectorizer:\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Vectores de características:\\n\", X.toarray())\n",
        "\n",
        "print(\"\\nPalabras del vocabulario:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "print('\\nVectores con palabras como columnas:')\n",
        "print(df)\n",
        "\n",
        "# Webscrapping-Ministerio-De-Turismo----------TfidfVectorizer-------------------------------------------------------------------------------\n",
        "\"\"\"\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://tableros.yvera.tur.ar/tablero_ODS/\"\n",
        "\n",
        "response = requests.get(url, verify=False)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "uls = soup.find_all('ul')\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for ul in uls:\n",
        "    descripciones = ul.find_all('p')\n",
        "    for d in descripciones:\n",
        "        corpus.append(d.text)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Ajustamos y transformamos nuestro corpus\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Características: \", vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nMatriz TF-IDF:\")\n",
        "print(X.toarray())\n",
        "\n",
        "# Resumen\n",
        "print(\"\\nVocabulario:\")\n",
        "print(vectorizer.vocabulary_)\n",
        "print(\"\\nIDF:\")\n",
        "print(vectorizer.idf_)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RHz4mnpnq1Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 3:\n",
        "\n",
        "Utilice la librería HashingVectorizer para obtener los vectores de\n",
        "características resultantes en los documentos anteriores:\n",
        "\n",
        "**¿Qué quiere decir que esta metodología es una técnica “sin estado”?**\n",
        "\n",
        "Significa que no mantiene ninguna información sobre el estado anterior (como un vocabulario). Esto significa que no puede proporcionar una forma de mapear desde las características a las palabras originales. Esto puede ser un inconveniente si necesitamos interpretar los vectores de características.\n",
        "\n",
        "**¿Cuándo puede ser una ventaja el uso de esta técnica?**\n",
        "\n",
        "Como no requiere que se mantenga un vocabulario, lo que puede ser muy útil en situaciones en las que el vocabulario puede ser muy grande y consumir mucha memoria.\n",
        "\n",
        "**¿En qué casos puede generar algunos problemas?**\n",
        "\n",
        "Cuando puede haber colisiones de hash y cuando no se puede mapear las características de vuelta a las palabras originales.\n",
        "\n",
        "**¿Explique el concepto de “matriz dispersa”?**\n",
        "\n",
        "La matriz dispersa es una matriz en la que la mayoría de los elementos son cero donde se puede tomar como criterio que la cantidad de elementos distintos de cero es aproximadamente igual al número de filas o columnas.\n",
        "\n",
        "**¿Qué rango numérico tienen los valores si se utiliza norm=None?**\n",
        "\n",
        "Cuando se utiliza HashingVectorizer con norm=None, el rango numérico de los valores resultantes es de 0 a n_features - 1, donde n_features es el tamaño del vector de características especificado."
      ],
      "metadata": {
        "id": "qpev_9JQeQre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "import PyPDF2\n",
        "\n",
        "#Vectorizacion Hash de PROYECTO DE LEY TURISMO SOCIAL 2004\n",
        "with open('/content/PROYECTO DE LEY TURISMO SOCIAL 2004.pdf', 'rb'):\n",
        "    lector = PyPDF2.PdfReader('PROYECTO DE LEY TURISMO SOCIAL 2004.pdf')\n",
        "\n",
        "texto = \"\"\n",
        "\n",
        "for i in range(len(lector.pages)):\n",
        "    pagina = lector.pages[i]\n",
        "    texto += pagina.extract_text()\n",
        "\n",
        "#Tokenizo el texto y elimino las stopwords, luego genero el texto sin las stopwords:\n",
        "palabras = nltk.word_tokenize(texto)\n",
        "palabras_sin_stopwords = [palabra for palabra in palabras if palabra.lower() not in stopwords_esp]\n",
        "texto_sin_stopwords = ' '.join(palabras_sin_stopwords)\n",
        "\n",
        "#Segmentación del texto sin stopwords:\n",
        "corpus = texto_sin_stopwords.split(\".\")\n",
        "\n",
        "#Este es el corpus:\n",
        "print(corpus)\n",
        "\n",
        "#Creo el Vectorizador Hash y aplico la transformación\n",
        "vectorizer = HashingVectorizer(n_features=10)\n",
        "vector = vectorizer.transform(corpus)\n",
        "\n",
        "# Imprimimos el resultado\n",
        "print(vector.toarray())\n",
        "\n",
        "# summarize the vector\n",
        "print('\\nForma del vector:')\n",
        "print(vector.shape)"
      ],
      "metadata": {
        "id": "LGn0LnwbemMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 4:\n",
        "\n",
        "**¿A qué hace referencia la bibliografía cuando indica que los métodos\n",
        "utilizados en los ejercicios anteriores no capturan la semántica y el contexto de las palabras.? De una explicación breve.**\n",
        "\n",
        "Cuando se dice que los métodos anteriores de vectorización no capturan la semántica y el contexto de las palabras es porque tratan a las palabras como entidades aisladas y no capturan el contexto o la relación entre esas palabras."
      ],
      "metadata": {
        "id": "dYhLc4PEemmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 5:\n",
        "\n",
        "Genere un código en python que permita tomar el resultado de una\n",
        "matriz de vectores de los ejercicios anteriores y extraiga las 10 similaridades de coseno mayores e imprima las palabras correspondientes en los casos que es\n",
        "posible.\n"
      ],
      "metadata": {
        "id": "r17bBYfjewMW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-dzw6sge5O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 6:\n",
        "\n",
        "Cargue el modelo Word2Vec y explore palabras y similitudes en\n",
        "palabras que sean de su interés. Busque posibilidades y combinaciones donde el\n",
        "uso de la aritmética de palabras de resultados semánticamente coherentes."
      ],
      "metadata": {
        "id": "uRy5cJvb6mEP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sUYGa2S-6smv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 7:\n",
        "\n",
        "Utilice el texto recuperado del autor Hernán Casciari de la práctica\n",
        "anterior para crear un corpus de texto, elimine las stopwords y utilice Word2Vec, GloVe (Global Vectors for Word Representation) y FastText, compare los\n",
        "resultados.\n"
      ],
      "metadata": {
        "id": "VHod8MVg6s-f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_kM-ddx461zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 8:\n",
        "\n",
        "Separe por párrafos u oraciones el texto recopilado de Casciari. Usar\n",
        "doc2vec sobre este corpus."
      ],
      "metadata": {
        "id": "BYJiAhKM61Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7wc6XHLS69HP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}