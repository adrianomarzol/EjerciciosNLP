{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 1:\n",
        "\n",
        "Escriba ejemplos de frase o busque ejemplos de párrafos de texto con diferentes estructuras, palabras y signos de puntuación. Utilice los códigos de One-Hot Encoding en las dos versiones que presenta el material expuesto en clase. Analice los resultados."
      ],
      "metadata": {
        "id": "bM2sPewkfdCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D337qe8XfYUp"
      },
      "outputs": [],
      "source": [
        "#importo las librerias necesarias\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Mis frases que luego divido en palabras\n",
        "fraseUno = \"Estaba en llamas cuando me acosté.\"\n",
        "fraseDos = \"La guitarra es un instrumento musical.\"\n",
        "\n",
        "palabrasUno = fraseUno.split()\n",
        "palabrasDos = fraseDos.split()\n",
        "\n",
        "#-----------Primera-Frase--------------------------------------------------------------------------\n",
        "\n",
        "#OneHotEncoding con OneHotEncoder\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "onehot_encodedUno = onehot_encoder.fit_transform(np.array(palabrasUno).reshape(-1, 1))\n",
        "\n",
        "print(onehot_encodedUno)\n",
        "\n",
        "for i, palabra in enumerate(palabrasUno):\n",
        "    print(f\"La palabra '{palabra}' se codificó como: {onehot_encodedUno[i]}\")\n",
        "\n",
        "#OneHotEncoding con pandas\n",
        "dataFrame = pd.DataFrame(palabrasUno, columns=['Palabras'])\n",
        "\n",
        "onehot_encodedDos = pd.get_dummies(dataFrame['Palabras'])\n",
        "\n",
        "print(onehot_encodedDos)\n",
        "\n",
        "for i, palabra in enumerate(palabrasUno):\n",
        "    print(f\"La palabra '{palabra}' se codificó como: {onehot_encodedDos.iloc[i].to_numpy()}\")\n",
        "\n",
        "#-----------Segunda-Frase--------------------------------------------------------------------------\n",
        "\n",
        "#OneHotEncoding con OneHotEncoder\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "onehot_encodedUno = onehot_encoder.fit_transform(np.array(palabrasDos).reshape(-1, 1))\n",
        "\n",
        "print(onehot_encodedUno)\n",
        "\n",
        "for i, palabra in enumerate(palabrasDos):\n",
        "    print(f\"La palabra '{palabra}' se codifió como: {onehot_encodedUno[i]}\")\n",
        "\n",
        "#OneHotEncoding con pandas\n",
        "dataFrame = pd.DataFrame(palabrasDos, columns=['Palabras'])\n",
        "\n",
        "onehot_encodedDos = pd.get_dummies(dataFrame['Palabras'])\n",
        "\n",
        "print(onehot_encodedDos)\n",
        "\n",
        "for i, palabra in enumerate(palabrasDos):\n",
        "    print(f\"La palbra '{palabra}' se codificó como: {onehot_encodedDos.iloc[i].to_numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 2:\n",
        "\n",
        "De los recursos propuestos en el último ejercicio de la unidad 1 con el texto obtenido del documento PROYECTO DE LEY TURISMO SOCIAL 2004.pdf y el texto de la metodología en la extracción de webscrapping del ministerio de turismo.\n",
        "\n",
        "Utilizar las bibliotecas de procesamiento de texto en python para representar estos documentos en forma de matrices numéricas utilizando tanto CountVectorizer como TfidfVectorizer, y luego comparar las diferencias entre las dos representaciones.\n",
        "\n",
        "Si es necesario eliminar en alguno de los casos las \"stop-words\"."
      ],
      "metadata": {
        "id": "igZZJjjxpkkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "bBQkxh09cL-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "dw7_D5fyZ7n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-features"
      ],
      "metadata": {
        "id": "BVb-77XXav8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords_esp = set(stopwords.words('spanish'))"
      ],
      "metadata": {
        "id": "bIJHTqqkf_kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROYECTO-DE-LEY-TURISMO-SOCIAL-2004.pdf-----Con-CountVectorizer-------------------------------------------------------------------------\n",
        "\"\"\"\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "with open('/content/PROYECTO DE LEY TURISMO SOCIAL 2004.pdf', 'rb'):\n",
        "    lector = PyPDF2.PdfReader('PROYECTO DE LEY TURISMO SOCIAL 2004.pdf')\n",
        "\n",
        "texto = \"\"\n",
        "\n",
        "for i in range(len(lector.pages)):\n",
        "    pagina = lector.pages[i]\n",
        "    texto += pagina.extract_text()\n",
        "\n",
        "#Tokenizo el texto y elimino las stopwords, luego genero el texto sin las stopwords:\n",
        "palabras = nltk.word_tokenize(texto)\n",
        "palabras_sin_stopwords = [palabra for palabra in palabras if palabra.lower() not in stopwords_esp]\n",
        "texto_sin_stopwords = ' '.join(palabras_sin_stopwords)\n",
        "\n",
        "#Segmentación del texto sin stopwords:\n",
        "corpus = texto_sin_stopwords.split(\".\")\n",
        "\n",
        "#Instancia de CountVectorizer:\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Vectores de características:\\n\", X.toarray())\n",
        "\n",
        "print(\"\\nPalabras del vocabulario:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "print('\\nVectores con palabras como columnas:')\n",
        "print(df)\n",
        "\"\"\"\n",
        "# Webscrapping-Ministerio-De-Turismo----------TfidfVectorizer--------------------------------------------------------------------------------\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://tableros.yvera.tur.ar/tablero_ODS/\"\n",
        "\n",
        "response = requests.get(url, verify=False)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "uls = soup.find_all('ul')\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for ul in uls:\n",
        "    descripciones = ul.find_all('p')\n",
        "    for d in descripciones:\n",
        "        corpus.append(d.text)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Ajustamos y transformamos nuestro corpus\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Características: \", vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nMatriz TF-IDF:\")\n",
        "print(X.toarray())\n",
        "\n",
        "# Resumen\n",
        "print(\"\\nVocabulario:\")\n",
        "print(vectorizer.vocabulary_)\n",
        "print(\"\\nIDF:\")\n",
        "print(vectorizer.idf_)"
      ],
      "metadata": {
        "id": "RHz4mnpnq1Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 3:\n",
        "\n",
        "Utilice la librería HashingVectorizer para obtener los vectores de\n",
        "características resultantes en los documentos anteriores:\n",
        "\n",
        "¿Qué quiere decir que esta metodología es una técnica “sin estado”?\n",
        "\n",
        "¿Cuándo puede ser una ventaja el uso de esta técnica?\n",
        "\n",
        "¿En qué casos puede generar algunos problemas?\n",
        "\n",
        "¿Explique el concepto de “matriz dispersa”?\n",
        "\n",
        "¿Qué rango numérico tienen los valores si se utiliza norm=None?"
      ],
      "metadata": {
        "id": "qpev_9JQeQre"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LGn0LnwbemMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 4:\n",
        "\n",
        "¿A qué hace referencia la bibliografía cuando indica que los métodos\n",
        "utilizados en los ejercicios anteriores no capturan la semántica y el contexto de\n",
        "las palabras.? De una explicación breve."
      ],
      "metadata": {
        "id": "dYhLc4PEemmi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wm5FXvETetw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 5:\n",
        "\n",
        "Genere un código en python que permita tomar el resultado de una\n",
        "matriz de vectores de los ejercicios anteriores y extraiga las 10 similaridades de coseno mayores e imprima las palabras correspondientes en los casos que es\n",
        "posible.\n"
      ],
      "metadata": {
        "id": "r17bBYfjewMW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-dzw6sge5O2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}